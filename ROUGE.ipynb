{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re as re\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DELETE before publication\n",
    "github_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "@Misc{acl-ocl,\n",
    "    author =       {Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan},\n",
    "    title =        {The ACL OCL Corpus: advancing Open science in Computational Linguistics},\n",
    "    howpublished = {arXiv},\n",
    "    year =         {2022},\n",
    "    url =          {https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/acl-publication-info.74k.v2.parquet')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"acl_id\": \"paper_ident\", # unique paper identifier\n",
    "                   \"url\": \"paper_url\", # Paper online abstract page URL.\n",
    "                   \"author\": \"paper_author\", # Author list.\n",
    "                   \"title\": \"paper_title\", # Paper title\n",
    "                   \"journal\": \"paper_venue\", # Venue abbreviation.\n",
    "                   \"year\": \"paper_year\", # Publication year\n",
    "                   \"month\": \"paper_month\", # Publication month.\n",
    "                   \"booktitle\": \"paper_booktitle\", # BibTeX booktitle field.\n",
    "                   \"address\": \"paper_address\", # BibTeX adress field\n",
    "                   \"publisher\": \"paper_publisher\", # BibTeX publisher field      \n",
    "                   \"pages\": \"paper_pages\", # BibTeX pages.\n",
    "                   \"full_text\": \"paper_text\",\n",
    "                   })\n",
    "\n",
    "df = df.drop(columns=[\"abstract\", \"corpus_paper_id\", \"pdf_hash\", \"doi\",\n",
    "                              \"numcitedby\", \"number\", \"volume\",  \n",
    "                              \"editor\", \"isbn\", \"ENTRYTYPE\",\"ID\", \"language\", \"note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_download'] = df['paper_text'].apply(lambda x: not x.strip() if isinstance(x, str) else True)\n",
    "\n",
    "df['error_download'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Scores\n",
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_prelim = df.copy()\n",
    "df_rouge_prelim[\"paper_rouge_prelim\"] = df_rouge_prelim[\"paper_text\"].str.contains(\"rouge\", case=False)\n",
    "\n",
    "df_rouge_prelim['paper_rouge_prelim'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Review\n",
    "#### ROUGE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(text):\n",
    "    pattern = r\"((?: -[a-z123](?: [a-z0-9.]{1,4})?){2,})\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_params = df_rouge_prelim.copy()\n",
    "df_rouge_params[\"paper_rouge_params\"] = df_rouge_params.apply(lambda row: extract_parameters(row['paper_text']) if row['paper_rouge_prelim'] else None, axis=1)\n",
    "\n",
    "\n",
    "df_rouge_params['paper_rouge_params'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_rouge_protocol = {\n",
    "    'stemming': r'\\b(?:stems?|stemming|stemmer|porter)\\b',\n",
    "    'tokenization': r'\\b(?:tokenized?|tokenizer|tokenization|pre-tokenized?|detokenized?)\\b',\n",
    "    'sentence_tokenization': r'sentence split|split sentence|sentence tokeniz|tokenize sentence',\n",
    "    'stopword_removal': r'\\b(?:stop( -)?words?)\\b',\n",
    "    'bootstrapping': r'(?:bootstrap|confidence (?:level|interval))'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_terms_near_rouge(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Find all occurrences of 'rouge' (case insensitive)\n",
    "        for match in re.finditer(r'rouge', text, re.IGNORECASE):\n",
    "            start, end = match.start(), match.end()\n",
    "            # Define a 500-character window around 'rouge'\n",
    "            window_start, window_end = max(0, start - 500), min(len(text), end + 500)\n",
    "            # Search for the term within this window\n",
    "            if re.search(pattern, text[window_start:window_end], re.IGNORECASE):\n",
    "                results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_protocol=df_rouge_params.copy()\n",
    "df_rouge_protocol['paper_rouge_protocol'] = df_rouge_protocol[df_rouge_protocol['paper_rouge_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_rouge(x, regex_rouge_protocol))\n",
    "\n",
    "df_rouge_protocol[\"paper_rouge_protocol\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_rouge_variants = {\n",
    "    'precision': r'\\b(?:precision)\\b',\n",
    "    'recall': r'\\b(?:recall)\\b',\n",
    "    'f-score': r'(?:\\b(?:f1?[- ]scores?|f1?[- ]measures?)\\b)| f-?1[^a-z0-9]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_variants=df_rouge_protocol.copy()\n",
    "df_rouge_variants['paper_rouge_variants'] = df_rouge_variants[df_rouge_variants['paper_rouge_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_rouge(x, regex_rouge_variants))\n",
    "\n",
    "df_rouge_variants['paper_rouge_variants'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_regex_pattern(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Search for the pattern in the entire text\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_rouge_packages = {\n",
    "    'DD/sacrerouge': r'sacrerouge',\n",
    "    'ND/easyrouge': r'easy.rouge|neural.{0,3}dialogue.{0,3}metrics',\n",
    "    'CW/sumeval': r'chakki.{0,3}works|sumeval',\n",
    "    'JG/pyrouegzh': r'py_rouge_zh',\n",
    "    'AR/gingo': r'asahi-research.{0,5}Gingo',\n",
    "    'DF/gerouge': r'gerouge',\n",
    "    'GL/seq2seq': r'seq2seq.{0,5}metrics.{0,5}rouge',\n",
    "    'GL/rougescore': r'rouge-score|google.research.{0,50}rouge',\n",
    "    'PT/files2rouge': r'files?2rouge',\n",
    "    'PC/pyrouge': r'pcyin',   \n",
    "    'KZ/rougepapier': r'rouge.papier',\n",
    "    'DI/pyrouge': r'py-rouge|diego999',    \n",
    "    'PT/pyrouge': r'pltrdy.{0,5}pyrouge',\n",
    "    'PT/rouge': r'pltrdy[^p]{0,5}rouge|pypi.{0,5}project.{0,5}rouge',\n",
    "    'AJ/pyrouge': r'andersjo',\n",
    "    'BZ/pyrouge': r'bheinzerling|pypi.{0,5}project.{0,5}pyrouge|pypi.{0,5}pyrouge',\n",
    "    'TG/pythonrouge': r'tagucci|pythonrouge',\n",
    "    'KG/rouge2': r'kavgan|rxnlp|rouge.2\\.0|jrouge|java rouge|kavita.ganesan.com',\n",
    "    'MS/rouge': r'nlg-eval|e2e-metrics|qgevalcap|nmtpytorch|pycocoevalcap|\\\\btylin\\\\b|coco-caption',\n",
    "    'github rouge': r'github.com.{0,50}rouge',\n",
    "    'unknown pyrouge': r'pyrouge',\n",
    "    'ROUGE-1.5.5': r'official rouge|rouge toolkit|rouge-?1\\.?5\\.?5|rouge.{0,15}1.?5.?5.?|rougeeval|berouge\\..{0,2}com|cly/.{0,2}rouge|isi\\.edu/.{0,2}rouge|isi\\.edu/.{0,2}licensed-sw/.{0,2}see/.{0,2}rouge'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_packages = df_rouge_variants.copy()\n",
    "# Applying the function to the DataFrame\n",
    "df_rouge_packages['paper_rouge_packages'] = df_rouge_packages[df_rouge_packages['paper_rouge_prelim'] == True]\\\n",
    "    ['paper_text'].apply(lambda x: search_for_regex_pattern(x, regex_rouge_packages))\n",
    "\n",
    "df_rouge_packages[\"paper_rouge_packages\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_packages.to_pickle(\"rouge_paper_review.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Review\n",
    "#### URL of code repository cited in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_packages = pd.read_pickle(\"rouge_paper_review.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_url = df_rouge_packages.copy()\n",
    "\n",
    "# regex for ALL codebases\n",
    "regex_codebases = r'https?://(?:www\\.)?(?:github\\.com|gitlab\\.com|bitbucket\\.org|sourceforge\\.net|google\\.code|code\\.google)[^\\s)]*(?<!\\.)'\n",
    "\n",
    "# Function to extract URLs from a text\n",
    "def extract_codebases(text):\n",
    "    return re.findall(regex_codebases, text)\n",
    "\n",
    "# Apply extract_codebases function to 'paper_text', store URLs in a list within each cell\n",
    "df_rouge_url[\"code_rouge_url\"] = df_rouge_url.apply(\n",
    "    lambda row: extract_codebases(row['paper_text']) if row['paper_rouge_prelim'] and pd.notnull(row['paper_text']) else [],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the code mention ROUGE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GitHub repository names from URLs\n",
    "def extract_github_repo_names(urls):\n",
    "    return [\"/\".join(urlparse(url).path.strip(\"/\").split(\"/\")[:2]) for url in urls if \"github.com\" in urlparse(url).netloc]\n",
    "\n",
    "# Apply the function to extract GitHub repository names only if 'paper_rouge_prelim' is True\n",
    "df_rouge_url['code_rouge_github'] = df_rouge_url.apply(lambda row: extract_github_repo_names(row['code_rouge_url']) if row['paper_rouge_prelim'] else [], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_reproducible = df_rouge_url.copy()\n",
    "\n",
    "# Initialize the 'reproducible' column as a nullable boolean\n",
    "df_rouge_reproducible['reproducible'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1: Check if both packages and params are not null\n",
    "condition_r1 = df_rouge_reproducible['paper_rouge_packages'].notna() & df_rouge_reproducible['paper_rougeparams'].notna()\n",
    "df_rouge_reproducible.loc[condition_r1, 'reproducible'] = True\n",
    "\n",
    "df_rouge_reproducible['reproducible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2: Check for no configuration packages\n",
    "# Define the no configuration packages list\n",
    "no_config_packages = ['Meteor_coco', 'pymeteor', 'nlgeval_meteor', 'nltk_meteor']\n",
    "\n",
    "def check_reproducibility(row):\n",
    "    # Only modify if reproducible is False or pd.NA\n",
    "    if row['reproducible'] is False or pd.isna(row['reproducible']):\n",
    "        # Check if paper_meteor_packages is a list and not empty or NA\n",
    "        if isinstance(row['paper_rouge_packages'], list) and row['paper_rouge_packages']:\n",
    "            # Check if any package in the list requires no configuration\n",
    "            if any(pkg in no_config_packages for pkg in row['paper_rouge_packages']):\n",
    "                return True\n",
    "    return row['reproducible']\n",
    "\n",
    "# Apply the function to update the 'reproducible' column\n",
    "df_rouge_reproducible['reproducible'] = df_rouge_reproducible.apply(check_reproducibility, axis=1)\n",
    "\n",
    "df_rouge_reproducible['reproducible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rouge_reproducible.to_pickle(\"rouge_paper_review.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df: pd.DataFrame, filepath: str = \"rouge_papers.jsonl.gz\") -> None:\n",
    "    \"\"\"\n",
    "    Save the DataFrame to a .jsonl.gz file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to save.\n",
    "    - filepath: The file path where the DataFrame should be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_json(filepath, orient=\"records\", lines=True, compression=\"gzip\")\n",
    "        print(f\"Dataset successfully saved to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save dataset: {e}\")\n",
    "\n",
    "save_dataset(df_rouge_reproducible, \"rouge_papers.jsonl.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
