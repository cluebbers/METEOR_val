{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re as re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DELETE before publication\n",
    "github_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "@Misc{acl-ocl,\n",
    "    author =       {Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan},\n",
    "    title =        {The ACL OCL Corpus: advancing Open science in Computational Linguistics},\n",
    "    howpublished = {arXiv},\n",
    "    year =         {2022},\n",
    "    url =          {https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>language</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>18022704</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/O02-2002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>chen-you-2002-study</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L02-1310</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8220988</td>\n",
       "      <td>8d5e31610bc82c2abc86bc20ceba684c97e66024</td>\n",
       "      <td>93</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>mihalcea-2002-bootstrapping</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R13-1042</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>16703040</td>\n",
       "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
       "      <td>10</td>\n",
       "      <td>https://aclanthology.org/R13-1042</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>jamison-gurevych-2013-headerless</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>aswani-gaizauskas-2005-aligning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L02-1309</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18078432</td>\n",
       "      <td>011e943b64a78dadc3440674419821ee080f0de3</td>\n",
       "      <td>12</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>suyaga-etal-2002-proposal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl_id                                           abstract  \\\n",
       "0  O02-2002  There is a need to measure word similarity whe...   \n",
       "1  L02-1310                                               None   \n",
       "2  R13-1042  Thread disentanglement is the task of separati...   \n",
       "3  W05-0819  In this paper, we describe a word alignment al...   \n",
       "4  L02-1309                                               None   \n",
       "\n",
       "                                           full_text  corpus_paper_id  \\\n",
       "0  There is a need to measure word similarity whe...         18022704   \n",
       "1                                               None          8220988   \n",
       "2  Thread disentanglement is the task of separati...         16703040   \n",
       "3  In this paper, we describe a word alignment al...          1215281   \n",
       "4                                               None         18078432   \n",
       "\n",
       "                                   pdf_hash  numcitedby  \\\n",
       "0  0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
       "1  8d5e31610bc82c2abc86bc20ceba684c97e66024          93   \n",
       "2  3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
       "3  b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "4  011e943b64a78dadc3440674419821ee080f0de3          12   \n",
       "\n",
       "                                                 url  \\\n",
       "0                  https://aclanthology.org/O02-2002   \n",
       "1  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "2                  https://aclanthology.org/R13-1042   \n",
       "3                  https://aclanthology.org/W05-0819   \n",
       "4  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "\n",
       "                                        publisher  \\\n",
       "0                                            None   \n",
       "1  European Language Resources Association (ELRA)   \n",
       "2                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "3       Association for Computational Linguistics   \n",
       "4  European Language Resources Association (ELRA)   \n",
       "\n",
       "                              address  year  ...   doi number volume journal  \\\n",
       "0                                None  2002  ...  None   None   None    None   \n",
       "1  Las Palmas, Canary Islands - Spain  2002  ...  None   None   None    None   \n",
       "2                    Hissar, Bulgaria  2013  ...  None   None   None    None   \n",
       "3                 Ann Arbor, Michigan  2005  ...  None   None   None    None   \n",
       "4  Las Palmas, Canary Islands - Spain  2002  ...  None   None   None    None   \n",
       "\n",
       "  editor  isbn      ENTRYTYPE                                ID language  note  \n",
       "0   None  None  inproceedings               chen-you-2002-study     None  None  \n",
       "1   None  None  inproceedings       mihalcea-2002-bootstrapping     None  None  \n",
       "2   None  None  inproceedings  jamison-gurevych-2013-headerless     None  None  \n",
       "3   None  None  inproceedings   aswani-gaizauskas-2005-aligning     None  None  \n",
       "4   None  None  inproceedings         suyaga-etal-2002-proposal     None  None  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('data/acl-publication-info.74k.v2.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acl_id             73285\n",
       "abstract           67669\n",
       "full_text          67455\n",
       "corpus_paper_id    73285\n",
       "pdf_hash           72076\n",
       "numcitedby         73285\n",
       "url                73285\n",
       "publisher          63166\n",
       "address            66093\n",
       "year               73285\n",
       "month              65962\n",
       "booktitle          71244\n",
       "author             72618\n",
       "title              73285\n",
       "pages              59478\n",
       "doi                29678\n",
       "number              1474\n",
       "volume              1840\n",
       "journal             2037\n",
       "editor                13\n",
       "isbn                1370\n",
       "ENTRYTYPE          73285\n",
       "ID                 73285\n",
       "language            3020\n",
       "note                 197\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_ident        67455\n",
       "paper_text         67455\n",
       "paper_url          67455\n",
       "paper_publisher    57851\n",
       "paper_address      60454\n",
       "paper_year         67455\n",
       "paper_month        60301\n",
       "paper_booktitle    65440\n",
       "paper_author       66888\n",
       "paper_title        67455\n",
       "paper_pages        57195\n",
       "paper_venue         2011\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"acl_id\": \"paper_ident\", # unique paper identifier\n",
    "                   \"url\": \"paper_url\", # Paper online abstract page URL.\n",
    "                   \"author\": \"paper_author\", # Author list.\n",
    "                   \"title\": \"paper_title\", # Paper title\n",
    "                   \"journal\": \"paper_venue\", # Venue abbreviation.\n",
    "                   \"year\": \"paper_year\", # Publication year\n",
    "                   \"month\": \"paper_month\", # Publication month.\n",
    "                   \"booktitle\": \"paper_booktitle\", # BibTeX booktitle field.\n",
    "                   \"address\": \"paper_address\", # BibTeX adress field\n",
    "                   \"publisher\": \"paper_publisher\", # BibTeX publisher field      \n",
    "                   \"pages\": \"paper_pages\", # BibTeX pages.\n",
    "                   \"full_text\": \"paper_text\",\n",
    "                   })\n",
    "\n",
    "df = df.drop(columns=[\"abstract\", \"corpus_paper_id\", \"pdf_hash\", \"doi\",\n",
    "                              \"numcitedby\", \"number\", \"volume\",  \n",
    "                              \"editor\", \"isbn\", \"ENTRYTYPE\",\"ID\", \"language\", \"note\"])\n",
    "\n",
    "df = df.dropna(subset=[\"paper_text\"])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU\n",
    "## Reproducibility\n",
    "### BLEU identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bleu_prelim = df.copy()\n",
    "df_bleu_prelim[\"paper_bleu_prelim\"] = df_bleu_prelim[\"paper_text\"].str.contains(\"bleu\", case=False)\n",
    "df_bleu_prelim['paper_bleu_prelim'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Review\n",
    "#### BLEU Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(text):\n",
    "    pattern = r\"((?: -[a-z123](?: [a-z0-9.]{1,4})?){2,})\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_bleu_params\n",
      " -1 -1                                                            7\n",
      " -s or -e                                                         3\n",
      " -1 -2 0                                                          2\n",
      " -2 -a                                                            1\n",
      " -1 lf -1 lf                                                      1\n",
      " -1 -s                                                            1\n",
      " -1 -1 ... -1 zu -2 0                                             1\n",
      " -1 -1 8 -1 -1 -1 ...                                             1\n",
      " -i -p 6060                                                       1\n",
      " -c 95 -m -n 4 -w 1.2.                                            1\n",
      " -o -n                                                            1\n",
      " -r and -y                                                        1\n",
      " -y -2                                                            1\n",
      " -s -e                                                            1\n",
      " -2 -1                                                            1\n",
      " -c -a -k                                                         1\n",
      " -3 -2 -1 0                                                       1\n",
      " -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1 -1 -1 0    1\n",
      " -i see -i                                                        1\n",
      " -1 and -2 poin                                                   1\n",
      " -e -a pojk                                                       1\n",
      " -a 22 -b 8 -q 8                                                  1\n",
      " -3 -2 -1 1                                                       1\n",
      " -a long -t                                                       1\n",
      " -2 or -3 is                                                      1\n",
      " -a and -a                                                        1\n",
      " -1 and -2 from                                                   1\n",
      " -o -v                                                            1\n",
      " -s -n -e                                                         1\n",
      " -3 to -2                                                         1\n",
      " -n 2 -m -u -c 95 -x -r 1000 -f                                   1\n",
      " -1 -1 c2 -1                                                      1\n",
      " -1 l -1 l -1 l                                                   1\n",
      " -3 -3                                                            1\n",
      " -s 2 -t                                                          1\n",
      " -h n -h n                                                        1\n",
      " -1 0.67 -1 0.73 -1 0.73 -1 0.79 -1                               1\n",
      " -1 0 -i                                                          1\n",
      " -b 0.03 -s 100                                                   1\n",
      " -1 -1 -1 -1 -1 1                                                 1\n",
      " -t -d 0                                                          1\n",
      " -2 -2                                                            1\n",
      " -3 and -2                                                        1\n",
      " -1 2 -2 1                                                        1\n",
      " -a -l                                                            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bleu_params = df_bleu_prelim.copy()\n",
    "df_bleu_params[\"paper_bleu_params\"] = df_bleu_params.apply(lambda row: extract_parameters(row['paper_text']) if row['paper_bleu_prelim'] else None, axis=1)\n",
    "\n",
    "# Display the filtered rows\n",
    "print(df_bleu_params['paper_bleu_params'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_regex_protocol = {\n",
    "    'ngrams': r\"\\bn-?grams?\\b\",\n",
    "    'precision': r\"\\bn-?gram\\sprecision\\b\",\n",
    "    'clipping': r\"\\bclipping\\b\",\n",
    "    'brevity_penalty': r\"\\bbrevity\\spenalty\\b|\\bBP\\b\",\n",
    "    'weights': r\"\\bweighting\\sof\\sn-?grams\\b\",\n",
    "    'smoothing': r\"\\bsmoothing\\b\",\n",
    "    'tokenization': r'\\b(?:tokenized?|tokenizer|tokenization|pre-tokenized?|detokenized?)\\b',\n",
    "    'case_normalization': r'\\b(?:case normalization|lowercased|case-insensitive|case sensitive)\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_terms_near_bleu(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Find all occurrences of 'meteor' (case insensitive)\n",
    "        for match in re.finditer(r'bleu', text, re.IGNORECASE):\n",
    "            start, end = match.start(), match.end()\n",
    "            # Define a 500-character window around 'meteor'\n",
    "            window_start, window_end = max(0, start - 500), min(len(text), end + 500)\n",
    "            # Search for the term within this window\n",
    "            if re.search(pattern, text[window_start:window_end], re.IGNORECASE):\n",
    "                results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_protocol=df_bleu_params.copy()\n",
    "df_bleu_protocol['paper_protocol'] = df_bleu_protocol[df_bleu_protocol['paper_bleu_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_bleu(x, bleu_regex_protocol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_protocol\n",
      "[]                                                               5297\n",
      "[ngrams]                                                         1271\n",
      "[tokenization]                                                    655\n",
      "[smoothing]                                                       313\n",
      "[case_normalization]                                              256\n",
      "                                                                 ... \n",
      "[smoothing, ngrams, clipping]                                       1\n",
      "[smoothing, precision, case_normalization, ngrams]                  1\n",
      "[smoothing, brevity_penalty, case_normalization]                    1\n",
      "[tokenization, ngrams, precision, brevity_penalty, smoothing]       1\n",
      "[tokenization, smoothing, ngrams, clipping]                         1\n",
      "Name: count, Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the filtered rows\n",
    "print(df_bleu_protocol[\"paper_protocol\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_regex_variants = {\n",
    "    'n_gram_precision': r'\\b(?:n-?gram precision|1-gram precision|2-gram precision|3-gram precision|4-gram precision)\\b',\n",
    "    'brevity_penalty': r'\\bbrevity penalty\\b|BP\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_variants\n",
      "[]                                     8491\n",
      "[brevity_penalty]                       295\n",
      "[n_gram_precision]                      199\n",
      "[brevity_penalty, n_gram_precision]     137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bleu_variants=df_bleu_protocol.copy()\n",
    "df_bleu_variants['paper_variants'] = df_bleu_variants[df_bleu_variants['paper_bleu_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_bleu(x, bleu_regex_variants))\n",
    "\n",
    "# Display the filtered rows\n",
    "print(df_bleu_variants['paper_variants'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_bleu_versions = {\n",
    "    'BLEU_original': r'\\bBLEU\\b.*?Papineni.*?ACL.*?2002',\n",
    "    'multi_bleu': r'multi-?bleu|multi_bleu',\n",
    "    'sacreBLEU': r'sacrebleu',\n",
    "    'nltk_bleu': r'nltk.*?bleu',\n",
    "    'mteval_v13a': r'mteval_v?13a',\n",
    "    'mteval_v14': r'mteval_v?14',\n",
    "    'BLEU_moses': r'bleu.*?moses',\n",
    "    'BLEU_nematus': r'bleu.*?nematus',\n",
    "    'BLEU_coco': r'bleu.*?coco',\n",
    "    'BLEU_pytorch': r'bleu.*?pytorch',\n",
    "    'BLEU_tensorflow': r'bleu.*?tensorflow',\n",
    "    'BLEU_fairseq': r'fairseq.*?bleu',\n",
    "    'BLEU_sacremoses': r'sacremoses.*?bleu',\n",
    "    'nematus_bleu': r'nematus.*?bleu',\n",
    "    'subword_nmt_bleu': r'subword-?nmt.*?bleu',\n",
    "    'sentence_bleu': r'sentence-?bleu',\n",
    "    'corpus_bleu': r'corpus-?bleu',\n",
    "    'smoothing_bleu': r'smoothing.*?bleu',\n",
    "    \"coco\": r'coco.*?bleu',\n",
    "    \"pybleu\": r'pybleu|py-bleu',\n",
    "    \"google_bleu\": r'google.*?bleu',\n",
    "    \"yisi_bleu\": r'yisi.*?bleu',\n",
    "    \"bertscore_bleu\": r'bertscore.*?bleu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_regex_pattern(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Search for the pattern in the entire text\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_bleu_packages\n",
      "[]                                           3786\n",
      "[BLEU_moses]                                 1023\n",
      "[smoothing_bleu]                              672\n",
      "[google_bleu]                                 484\n",
      "[BLEU_moses, smoothing_bleu]                  482\n",
      "[google_bleu, BLEU_moses]                     144\n",
      "[sacreBLEU]                                   144\n",
      "[BLEU_pytorch]                                141\n",
      "[BLEU_fairseq]                                135\n",
      "[bertscore_bleu]                              112\n",
      "[nltk_bleu]                                    93\n",
      "[multi_bleu, BLEU_moses]                       84\n",
      "[BLEU_coco, coco]                              79\n",
      "[google_bleu, smoothing_bleu]                  53\n",
      "[multi_bleu]                                   50\n",
      "[sacreBLEU, BLEU_fairseq]                      49\n",
      "[BLEU_fairseq, smoothing_bleu]                 48\n",
      "[BLEU_original]                                47\n",
      "[google_bleu, BLEU_moses, smoothing_bleu]      46\n",
      "[BLEU_tensorflow]                              46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bleu_packages = df_bleu_variants.copy()\n",
    "# Applying the function to the DataFrame\n",
    "df_bleu_packages['paper_bleu_packages'] = df_bleu_packages[df_bleu_packages['paper_bleu_prelim'] == True]\\\n",
    "    ['paper_text'].apply(lambda x: search_for_regex_pattern(x, regex_bleu_versions))\n",
    "    \n",
    "print(df_bleu_packages[\"paper_bleu_packages\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review\n",
    "#### URL of code repository cited in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_bleu_url\n",
      "https://github.com/                          239\n",
      "https://github.com/moses-smt/                 31\n",
      "https://github.com/pytorch/fairseq            26\n",
      "https://github.com/facebookresearch/          25\n",
      "https://github.com/huggingface/               17\n",
      "https://github.com/moses-smt/mosesdecoder     16\n",
      "https://github.com/fxsjy/jieba                15\n",
      "https://github.com/google/sentencepiece       14\n",
      "https://github.com/tensorflow/                13\n",
      "https://github.com/pytorch/fairseq/           13\n",
      "https://github.com/google-research/           11\n",
      "https://github.com/OpenNMT/OpenNMT-py          8\n",
      "https://github.com/mjpost/sacrebleu            8\n",
      "https://github.com/rsennrich/nematus           8\n",
      "https://github.com/microsoft/                  8\n",
      "http://code.google.com/p/giza-pp/              7\n",
      "https://github.com/rsennrich/                  6\n",
      "https://code.google.com/archive/p/             6\n",
      "https://github.com/awslabs/sockeye             6\n",
      "https://github.com/UFAL-DSG/tgen               6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_bleu_url = df_bleu_packages.copy()\n",
    "\n",
    "# regex for codebases\n",
    "regex_codebases = r'https?://(?:www\\.)?(?:github\\.com|gitlab\\.com|bitbucket\\.org|sourceforge\\.net|google\\.code|code\\.google)[^\\s)]*(?<!\\.)'\n",
    "\n",
    "# Function to extract URLs from a text\n",
    "def extract_codebases(text):\n",
    "    return re.findall(regex_codebases, text)\n",
    "\n",
    "df_bleu_url[\"code_bleu_url\"] = df_bleu_url[df_bleu_url['paper_bleu_prelim'] == True]['paper_text'].apply(extract_codebases)\n",
    "\n",
    "df_bleu_url = df_bleu_url.explode('code_bleu_url').dropna(subset=['code_bleu_url']).reset_index(drop=True)\n",
    "\n",
    "print(df_bleu_url[\"code_bleu_url\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the code mention BLEU?\n",
    "##### Check if repository is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_code = df_bleu_url.copy()\n",
    "\n",
    "import requests\n",
    "# Function to check if a URL exists\n",
    "def url_exists(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "# Check if each URL is active and add the result to a new column in the DataFrame\n",
    "df_bleu_code['active'] = df_bleu_code['code_bleu_url'].apply(url_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Github API to search for BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the necessary part of the GitHub URL for the API request\n",
    "def extract_github_repo_name(url):\n",
    "    # Splitting the URL and extracting the repository part\n",
    "    # Typical GitHub URL format: https://github.com/username/repository\n",
    "    parts = url.split('/')\n",
    "    if len(parts) > 4 and 'github.com' in parts:\n",
    "        # Extracting the username and repository name\n",
    "        return '/'.join(parts[3:5])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'URLs' column\n",
    "df_bleu_code['GitHub_Repo'] = df_bleu_code['code_bleu_url'].apply(extract_github_repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_code_check = df_bleu_code.copy()\n",
    "\n",
    "import time\n",
    "\n",
    "# Function to search a GitHub repository for a keyword\n",
    "def search_github_repo(repo_name, keyword='bleu', token=github_token):\n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    search_url = f'https://api.github.com/search/code?q={keyword}+repo:{repo_name}'\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['total_count']\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.json().get('message', 'No message')}\")\n",
    "        return None\n",
    "\n",
    "# Apply the search function to each GitHub repository, respecting the rate limit\n",
    "search_results = []\n",
    "for i, repo_name in enumerate(df_bleu_code_check['GitHub_Repo']):\n",
    "     # Only proceed if the repository is active\n",
    "    if df_bleu_code_check.loc[i, 'active']:\n",
    "        contains_rouge = search_github_repo(repo_name)\n",
    "        search_results.append(contains_rouge)\n",
    "    else:\n",
    "        search_results.append(False)\n",
    "    # Respect the rate limit\n",
    "    if i % 9 == 8:  # After every 9th request\n",
    "        time.sleep(60)\n",
    "        \n",
    "# Add the search results to the 'code_rouge_prelim' column\n",
    "df_bleu_code_check['code_bleu_prelim'] = search_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
