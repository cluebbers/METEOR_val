{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re as re\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DELETE before publication\n",
    "github_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "@Misc{acl-ocl,\n",
    "    author =       {Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan},\n",
    "    title =        {The ACL OCL Corpus: advancing Open science in Computational Linguistics},\n",
    "    howpublished = {arXiv},\n",
    "    year =         {2022},\n",
    "    url =          {https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/acl-publication-info.74k.v2.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"acl_id\": \"paper_ident\", # unique paper identifier\n",
    "                   \"url\": \"paper_url\", # Paper online abstract page URL.\n",
    "                   \"author\": \"paper_author\", # Author list.\n",
    "                   \"title\": \"paper_title\", # Paper title\n",
    "                   \"journal\": \"paper_venue\", # Venue abbreviation.\n",
    "                   \"year\": \"paper_year\", # Publication year\n",
    "                   \"month\": \"paper_month\", # Publication month.\n",
    "                   \"booktitle\": \"paper_booktitle\", # BibTeX booktitle field.\n",
    "                   \"address\": \"paper_address\", # BibTeX adress field\n",
    "                   \"publisher\": \"paper_publisher\", # BibTeX publisher field      \n",
    "                   \"pages\": \"paper_pages\", # BibTeX pages.\n",
    "                   \"full_text\": \"paper_text\",\n",
    "                   })\n",
    "\n",
    "df = df.drop(columns=[\"abstract\", \"corpus_paper_id\", \"pdf_hash\", \"doi\",\n",
    "                              \"numcitedby\", \"number\", \"volume\",  \n",
    "                              \"editor\", \"isbn\", \"ENTRYTYPE\",\"ID\", \"language\", \"note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_download'] = df['paper_text'].apply(lambda x: not x.strip() if isinstance(x, str) else True)\n",
    "\n",
    "df['error_download'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU\n",
    "## Reproducibility\n",
    "### BLEU identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_prelim = df.copy()\n",
    "df_bleu_prelim[\"paper_bleu_prelim\"] = df_bleu_prelim[\"paper_text\"].str.contains(\"bleu\", case=False)\n",
    "\n",
    "df_bleu_prelim['paper_bleu_prelim'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Review\n",
    "#### BLEU Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(text):\n",
    "    pattern = r\"((?: -[a-z123](?: [a-z0-9.]{1,4})?){2,})\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_params = df_bleu_prelim.copy()\n",
    "df_bleu_params[\"paper_bleu_params\"] = df_bleu_params.apply(lambda row: extract_parameters(row['paper_text']) if row['paper_bleu_prelim'] else None, axis=1)\n",
    "\n",
    "df_bleu_params['paper_bleu_params'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_regex_protocol = {\n",
    "    'ngrams': r\"\\bn-?grams?\\b\",\n",
    "    'precision': r\"\\bn-?gram\\sprecision\\b\",\n",
    "    'clipping': r\"\\bclipping\\b\",\n",
    "    'brevity_penalty': r\"\\bbrevity\\spenalty\\b|\\bBP\\b\",\n",
    "    'weights': r\"\\bweighting\\sof\\sn-?grams\\b\",\n",
    "    'smoothing': r\"\\bsmoothing\\b\",\n",
    "    'tokenization': r'\\b(?:tokenized?|tokenizer|tokenization|pre-tokenized?|detokenized?)\\b',\n",
    "    'case_normalization': r'\\b(?:case normalization|lowercased|case-insensitive|case sensitive)\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_terms_near_bleu(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Find all occurrences of 'bleu' (case insensitive)\n",
    "        for match in re.finditer(r'bleu', text, re.IGNORECASE):\n",
    "            start, end = match.start(), match.end()\n",
    "            # Define a 500-character window around 'bleu'\n",
    "            window_start, window_end = max(0, start - 500), min(len(text), end + 500)\n",
    "            # Search for the term within this window\n",
    "            if re.search(pattern, text[window_start:window_end], re.IGNORECASE):\n",
    "                results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_protocol=df_bleu_params.copy()\n",
    "df_bleu_protocol['paper_bleu_protocol'] = df_bleu_protocol[df_bleu_protocol['paper_bleu_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_bleu(x, bleu_regex_protocol))\n",
    "\n",
    "df_bleu_protocol[\"paper_bleu_protocol\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_regex_variants = {\n",
    "    'n_gram_precision': r'\\b(?:n-?gram precision|1-gram precision|2-gram precision|3-gram precision|4-gram precision)\\b',\n",
    "    'brevity_penalty': r'\\bbrevity penalty\\b|BP\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_variants=df_bleu_protocol.copy()\n",
    "df_bleu_variants['paper_variants'] = df_bleu_variants[df_bleu_variants['paper_bleu_prelim'] == True]['paper_text'].apply(lambda x: search_terms_near_bleu(x, bleu_regex_variants))\n",
    "\n",
    "df_bleu_variants['paper_variants'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_regex_pattern(text, regex_dict):\n",
    "    results = []\n",
    "    for term, pattern in regex_dict.items():\n",
    "        # Search for the pattern in the entire text\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            results.append(term)\n",
    "    return list(set(results))  # Return unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_bleu_versions = {\n",
    "    'BLEU_original': r'\\bBLEU\\b.*?Papineni.*?ACL.*?2002',\n",
    "    'multi_bleu': r'multi-?bleu|multi_bleu',\n",
    "    'sacreBLEU': r'sacrebleu',\n",
    "    'nltk_bleu': r'nltk.*?bleu',\n",
    "    'mteval_v13a': r'mteval_v?13a',\n",
    "    'mteval_v14': r'mteval_v?14',\n",
    "    'BLEU_moses': r'bleu.*?moses',\n",
    "    'BLEU_nematus': r'bleu.*?nematus',\n",
    "    'BLEU_coco': r'bleu.*?coco',\n",
    "    'BLEU_pytorch': r'bleu.*?pytorch',\n",
    "    'BLEU_tensorflow': r'bleu.*?tensorflow',\n",
    "    'BLEU_fairseq': r'fairseq.*?bleu',\n",
    "    'BLEU_sacremoses': r'sacremoses.*?bleu',\n",
    "    'nematus_bleu': r'nematus.*?bleu',\n",
    "    'subword_nmt_bleu': r'subword-?nmt.*?bleu',\n",
    "    'sentence_bleu': r'sentence-?bleu',\n",
    "    'corpus_bleu': r'corpus-?bleu',\n",
    "    'smoothing_bleu': r'smoothing.*?bleu',\n",
    "    \"coco\": r'coco.*?bleu',\n",
    "    \"pybleu\": r'pybleu|py-bleu',\n",
    "    \"google_bleu\": r'google.*?bleu',\n",
    "    \"yisi_bleu\": r'yisi.*?bleu',\n",
    "    \"bertscore_bleu\": r'bertscore.*?bleu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_packages = df_bleu_variants.copy()\n",
    "# Applying the function to the DataFrame\n",
    "df_bleu_packages['paper_bleu_packages'] = df_bleu_packages[df_bleu_packages['paper_bleu_prelim'] == True]\\\n",
    "    ['paper_text'].apply(lambda x: search_for_regex_pattern(x, regex_bleu_versions))\n",
    "    \n",
    "df_bleu_packages[\"paper_bleu_packages\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review\n",
    "#### URL of code repository cited in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_url = df_bleu_packages.copy()\n",
    "\n",
    "# regex for codebases\n",
    "regex_codebases = r'https?://(?:www\\.)?(?:github\\.com|gitlab\\.com|bitbucket\\.org|sourceforge\\.net|google\\.code|code\\.google)[^\\s)]*(?<!\\.)'\n",
    "\n",
    "# Function to extract URLs from a text\n",
    "def extract_codebases(text):\n",
    "    return re.findall(regex_codebases, text)\n",
    "\n",
    "# Apply extract_codebases function to 'paper_text', store URLs in a list within each cell\n",
    "df_bleu_url[\"code_bleu_url\"] = df_bleu_url.apply(\n",
    "    lambda row: extract_codebases(row['paper_text']) if row['paper_bleu_prelim'] and pd.notnull(row['paper_text']) else [],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the code mention BLEU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GitHub repository names from URLs\n",
    "def extract_github_repo_names(urls):\n",
    "    return [\"/\".join(urlparse(url).path.strip(\"/\").split(\"/\")[:2]) for url in urls if \"github.com\" in urlparse(url).netloc]\n",
    "\n",
    "# Apply the function to extract GitHub repository names only if 'paper_bleu_prelim' is True\n",
    "df_bleu_url['code_bleu_github'] = df_bleu_url.apply(lambda row: extract_github_repo_names(row['code_bleu_url']) if row['paper_bleu_prelim'] else [], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_reproducible = df_bleu_url.copy()\n",
    "\n",
    "# Initialize the 'reproducible' column as a nullable boolean\n",
    "df_bleu_reproducible['reproducible'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1: Check if both packages and params are not null\n",
    "condition_r1 = df_bleu_reproducible['paper_bleu_packages'].notna() & df_bleu_reproducible['paper_bleu_params'].notna()\n",
    "df_bleu_reproducible.loc[condition_r1, 'reproducible'] = True\n",
    "\n",
    "df_bleu_reproducible['reproducible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2: Check for no configuration packages\n",
    "# Define the no configuration packages list\n",
    "no_config_packages = ['Meteor_coco', 'pymeteor', 'nlgeval_meteor', 'nltk_meteor']\n",
    "\n",
    "def check_reproducibility(row):\n",
    "    # Only modify if reproducible is False or pd.NA\n",
    "    if row['reproducible'] is False or pd.isna(row['reproducible']):\n",
    "        # Check if paper_meteor_packages is a list and not empty or NA\n",
    "        if isinstance(row['paper_bleu_packages'], list) and row['paper_bleu_packages']:\n",
    "            # Check if any package in the list requires no configuration\n",
    "            if any(pkg in no_config_packages for pkg in row['paper_bleu_packages']):\n",
    "                return True\n",
    "    return row['reproducible']\n",
    "\n",
    "# Apply the function to update the 'reproducible' column\n",
    "df_bleu_reproducible['reproducible'] = df_bleu_reproducible.apply(check_reproducibility, axis=1)\n",
    "\n",
    "df_bleu_reproducible['reproducible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bleu_reproducible.to_pickle(\"bleu_paper_review.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df: pd.DataFrame, filepath: str = \"bleu_papers.jsonl.gz\") -> None:\n",
    "    \"\"\"\n",
    "    Save the DataFrame to a .jsonl.gz file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to save.\n",
    "    - filepath: The file path where the DataFrame should be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_json(filepath, orient=\"records\", lines=True, compression=\"gzip\")\n",
    "        print(f\"Dataset successfully saved to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save dataset: {e}\")\n",
    "\n",
    "save_dataset(df_bleu_reproducible, \"bleu_papers.jsonl.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
